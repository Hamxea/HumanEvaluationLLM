

# Evaluating Activity Models

## Link: https://cergdigitaltwinchat.idi.ntnu.no/

## Project Description:
Welcome to the Human Evaluation of Large Language Models platform, designed to evaluate the performance of advanced AI models fine-tuned with domain-specific data. Our focus is on enhancing the capabilities of models like LLaMA, Mistral, and StableBeluga using specialized datasets from the Cardiac Exercise Research Group (CERG) at NTNU. This fine-tuning process aims to improve the models' accuracy and relevance in generating responses related to activity-health, fitness, and well-being. Compare outputs from various models and gain insights into their effectiveness in the healthcare sector.

## Evaluation Steps:
### 1. Enter Your Email & Expert Status: Provide your email and select your expertise status. Click "Submit" to receive a unique evaluation code.
### 2. Evaluation Code: This code will be sent to your email and displayed on the screen. NOTE: Use this code to resume your evaluation from where you stop later if needed.
### 3. Start Evaluation: Read the project description and proceed with the evaluation.
### 4. Compare Responses: For each prompt, review two responses (A and B) generated by different models.
### 5. Select the Best Response: Choose the better response using the radio buttons ("A is better" or "B is better").
### 6. Rate Your Satisfaction: Rate your satisfaction with the chosen response on a scale from 1 (Very Dissatisfied) to 5 (Very Satisfied).
### 7. Provide Feedback (Optional): Add any additional feedback in the text area provided.
### 8. Submit Evaluations: Once all evaluations are complete, click "Submit All Evaluations."


Thank you for your participation! Your feedback is essential for improving AI models in the healthcare sector.

