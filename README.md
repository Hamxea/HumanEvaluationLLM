https://cergdigitaltwinchat.idi.ntnu.no/

Evaluating Activity Models
Project Description:
Welcome to the Human Evaluation of Large Language Models platform, designed to evaluate the performance of advanced AI models fine-tuned with domain-specific data. Our focus is on enhancing the capabilities of models like LLaMA, Mistral, and StableBeluga using specialized datasets from the Cardiac Exercise Research Group (CERG) at NTNU. This fine-tuning process aims to improve the models' accuracy and relevance in generating responses related to activity-health, fitness, and well-being. Compare outputs from various models and gain insights into their effectiveness in the healthcare sector.

Evaluation Steps:
Enter Your Email & Expert Status: Provide your email and select your expertise status. Click "Submit" to receive a unique evaluation code.
Evaluation Code: This code will be sent to your email and displayed on the screen. NOTE: Use this code to resume your evaluation from where you stop later if needed.
Start Evaluation: Read the project description and proceed with the evaluation.
Compare Responses: For each prompt, review two responses (A and B) generated by different models.
Select the Best Response: Choose the better response using the radio buttons ("A is better" or "B is better").
Rate Your Satisfaction: Rate your satisfaction with the chosen response on a scale from 1 (Very Dissatisfied) to 5 (Very Satisfied).
Provide Feedback (Optional): Add any additional feedback in the text area provided.
Navigate Between Models: Use "Previous Model" and "Next Model" buttons to evaluate different models.
Submit Evaluations: Once all evaluations are complete, click "Submit All Evaluations."
Thank you for your participation! Your feedback is essential for improving AI models in the healthcare sector.

Please enter your email and select your expert status before proceeding.
